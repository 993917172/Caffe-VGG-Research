{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Caffe Training Tool"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Import Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Caffe Directory Setting\n",
    "caffe_root = '../'\n",
    "\n",
    "import sys \n",
    "import os\n",
    "import os.path as osp\n",
    "import numpy as np\n",
    "import codecs\n",
    "import random\n",
    "\n",
    "# Import Caffe Python Library\n",
    "sys.path.append(caffe_root + 'python')\n",
    "import caffe \n",
    "\n",
    "# Import Custom Library\n",
    "sys.path.append(\"utils\")\n",
    "sys.path.append(\"utils/layers\")\n",
    "sys.path.append(\"utils/nets\")\n",
    "\n",
    "from multilabel_datalayers import *\n",
    "from pynet import *\n",
    "import tools"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Setup Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Dataset Directory Setting\n",
    "dataset_path = \"../arg-models/text-renderer/output/street_en_ratio\"\n",
    "lexicon_path = \"../arg-models/text-renderer/list/street_en_list.txt\"\n",
    "\n",
    "# Model Saving Directory Setting\n",
    "model_folder = \"street_en_ratio_1000/\"\n",
    "if os.path.isdir(model_folder+\"/snapshot\") is False:\n",
    "    os.makedirs(model_folder+\"/snapshot\")\n",
    "\n",
    "# Network Chosen\n",
    "network_idx = 1\n",
    "network_mode = [multilabel_bvlc, multilabel_vgg_dictnet, multilabel_large_vgg, multilabel_vgg16]\n",
    "network = network_mode[network_idx]\n",
    "\n",
    "# Training & Testing File\n",
    "createFile = True\n",
    "\n",
    "# Hyper-Parameters\n",
    "policy = \"step\"\n",
    "batch_sz = 32\n",
    "img_shape = [32, 100, 1]\n",
    "img_channel = 1\n",
    "    \n",
    "# Pre-Trained Model\n",
    "hasPretrained = False\n",
    "pretrained_model = '../models/bvlc_reference_caffenet/bvlc_reference_caffenet.caffemodel' #model_folder + 'vgg16.caffemodel'\n",
    "\n",
    "# Caffe Mode Set\n",
    "caffe.set_mode_gpu()\n",
    "caffe.set_device(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Load Classes and Lexicon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "classes: [u'FRAZZOLI' u'ST' u'TEDRAKE' u'ASADA' u'AVE' u'BREAZEAL' u'DUBOWSKY'\n",
      " u'HERR' u'HOBURG' u'HOGAN' u'IAGNEMMA' u'KAELBLING' u'LOZANO' u'KIM'\n",
      " u'REIF' u'WALTZ' u'PERAIRE' u'CHANDRA' u'KASAN' u'MICALI' u'CHEN' u'HOSOI'\n",
      " u'RUS' u'HOW' u'ROY' u'SHAH' u'WILLIAMS' u'LEONARD' u'BROOKS' u'KARAMAN']\n",
      "-----------------------\n",
      "name: [u'FRAZZOLI' u'TEDRAKE' u'ASADA' u'BREAZEAL' u'DUBOWSKY' u'HERR' u'HOBURG'\n",
      " u'HOGAN' u'IAGNEMMA' u'KAELBLING' u'LOZANO' u'KIM' u'REIF' u'WALTZ'\n",
      " u'PERAIRE' u'CHANDRA' u'KASAN' u'MICALI' u'CHEN' u'HOSOI' u'RUS' u'HOW'\n",
      " u'ROY' u'SHAH' u'WILLIAMS' u'LEONARD' u'BROOKS' u'KARAMAN']\n"
     ]
    }
   ],
   "source": [
    "pair_list = np.array([line.rstrip(\"\\n\").split(\",\") for line in codecs.open(lexicon_path, 'r', 'utf8').readlines()])\n",
    "\n",
    "classes = pair_list.flatten() \n",
    "\n",
    "distinct_classes = np.array([c for e,c in enumerate(classes) if c not in classes[:e]])\n",
    "pair_name = pair_list[...,0]\n",
    "    \n",
    "print \"classes:\", distinct_classes\n",
    "print \"-----------------------\"\n",
    "print \"name:\", pair_name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Create Training & Testing Data Paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "File Created!\n"
     ]
    }
   ],
   "source": [
    "if createFile is True:\n",
    "    \n",
    "    content = []\n",
    "    with codecs.open(os.path.join(dataset_path, \"path.txt\"), 'w', 'utf8') as p:\n",
    "        print os.path.isdir(dataset_path)\n",
    "        for dirPath, dirNames, fileNames in os.walk(dataset_path):\n",
    "            for file in fileNames:\n",
    "                if file.endswith('.jpg') is False:\n",
    "                    continue\n",
    "                path = os.path.join(dirPath, file)\n",
    "                \n",
    "                label = unicode(dirPath.split(\"/\")[-1], \"utf-8\")\n",
    "                label_idx = np.where(pair_name == label)[0][0]          \n",
    "                labels = pair_list[label_idx]\n",
    "                labels_each_num = [str(np.where(distinct_classes == label)[0][0]) for label in labels]            \n",
    "\n",
    "                out = unicode(path, \"utf-8\")+\",\"+','.join(labels_each_num)\n",
    "                content.append(out)\n",
    "                p.write(out)\n",
    "\n",
    "\n",
    "    random.shuffle(content)    \n",
    "    \n",
    "    with codecs.open(os.path.join(dataset_path, \"train.txt\"), 'w', 'utf8') as train:\n",
    "        train.write('\\n'.join(content[:int(len(content)*0.9)]))\n",
    "        \n",
    "    with codecs.open(os.path.join(dataset_path, \"val.txt\"), 'w', 'utf8') as val:\n",
    "        val.write('\\n'.join(content[int(len(content)*0.9):]))\n",
    "\n",
    "    print \"File Created!\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Write Prototxt Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Writing Solver Prototxt by tools.py\n",
    "\n",
    "solverprototxt = tools.CaffeSolver(trainnet_prototxt_path = osp.join(model_folder, \"trainnet.prototxt\"), testnet_prototxt_path = osp.join(model_folder, \"valnet.prototxt\"))\n",
    "\n",
    "solverprototxt.sp['display'] = \"50\"\n",
    "solverprototxt.sp['base_lr'] = \"0.001\"\n",
    "solverprototxt.sp['snapshot'] = \"50\"\n",
    "solverprototxt.sp['test_interval'] = \"2000\"\n",
    "solverprototxt.sp['snapshot_prefix'] = \"\\\"\"+osp.join(model_folder, \"snapshot\")+\"\\\"\"\n",
    "\n",
    "if policy == \"step\":\n",
    "    solverprototxt.sp['lr_policy'] = \"\\\"step\\\"\"\n",
    "    solverprototxt.sp['stepsize'] = \"100\"\n",
    "\n",
    "solverprototxt.write(osp.join(model_folder, 'solver.prototxt'))\n",
    "\n",
    "# Writing Trainnet & Valnet Prototxt\n",
    "with open(osp.join(model_folder, 'trainnet.prototxt'), 'w') as f:\n",
    "    data_layer_params = dict(batch_size = batch_sz, im_shape = img_shape, split = 'train', data_folder = dataset_path, lexicon = lexicon_path, channel=img_channel)\n",
    "    f.write(network(data_layer_params, len(distinct_classes)))\n",
    "\n",
    "with open(osp.join(model_folder, 'valnet.prototxt'), 'w') as f:\n",
    "    data_layer_params = dict(batch_size = batch_sz, im_shape = img_shape, split = 'val', data_folder = dataset_path, lexicon = lexicon_path, channel=img_channel)\n",
    "    f.write(network(data_layer_params, len(distinct_classes)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Train a net."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BatchLoader initialized with 25325 images\n",
      "BatchLoader initialized with 2814 images\n",
      "5.78744506836\n",
      "5.91815948486\n"
     ]
    }
   ],
   "source": [
    "solver = caffe.SGDSolver(osp.join(model_folder, 'solver.prototxt'))\n",
    "if hasPretrained:\n",
    "    solver.net.copy_from(pretrained_model)\n",
    "    \n",
    "for itt in range(int(solverprototxt.sp['max_iter'])/500):\n",
    "    solver.step(50)\n",
    "    print solver.net.blobs['loss'].data"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "description": "Multilabel classification on PASCAL VOC using a Python data layer.",
  "example_name": "Multilabel Classification with Python Data Layer",
  "include_in_docs": true,
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  },
  "priority": 5
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
